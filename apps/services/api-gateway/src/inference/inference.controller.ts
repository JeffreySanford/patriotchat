import {
  Controller,
  Get,
  Post,
  Body,
  HttpException,
  HttpStatus,
  UseGuards,
} from '@nestjs/common';
import { JwtAuthGuard } from '../auth/jwt-auth.guard';
import {
  InferenceModelsResponse,
  InferenceGenerateRequest,
  InferenceGenerateResponse,
} from '../types/api.dto';

@Controller('inference')
export class InferenceController {
  @Get('models')
  async getModels(): Promise<InferenceModelsResponse> {
    console.log('InferenceController: getModels called');
    // Return mock models directly
    const models = [
      {
        id: 'llama2',
        name: 'llama2',
        description: 'Llama 2 language model',
        provider: 'Meta',
      },
      {
        id: 'mistral',
        name: 'mistral',
        description: 'Mistral language model',
        provider: 'Mistral AI',
      },
      {
        id: 'neural-chat',
        name: 'neural-chat',
        description: 'Neural Chat language model',
        provider: 'Intel',
      },
    ];
    console.log('InferenceController: returning models:', models);
    return { models };
  }

  @Post('generate')
  async generateInference(
    @Body() body: InferenceGenerateRequest,
  ): Promise<InferenceGenerateResponse> {
    try {
      console.log('InferenceController: generateInference called with body:', body);
      const modelId = body.model;
      if (!modelId) {
        throw new HttpException(
          'Model is required',
          HttpStatus.BAD_REQUEST,
        );
      }
      // Return mock response
      const result: InferenceGenerateResponse = {
        prompt: body.prompt,
        text: `Mock response from ${modelId}: This is a simulated inference result for the prompt: "${body.prompt}". In a production environment, this would be generated by the actual LLM service.`,
        model: modelId,
        tokens: 42,
        duration: 150,
      };
      console.log('InferenceController: returning result:', result);
      return result;
    } catch (err: unknown) {
      console.error('InferenceController: error in generateInference:', err);
      const error = err as Record<string, unknown>;
      throw new HttpException(
        (error.message as string) || 'Inference generation failed',
        HttpStatus.BAD_GATEWAY,
      );
    }
  }
}
