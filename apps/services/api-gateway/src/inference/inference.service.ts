import { Injectable, Inject } from '@nestjs/common';
import { HttpService } from '@nestjs/axios';
import { Observable, of } from 'rxjs';
import { map, catchError, tap } from 'rxjs/operators';
import { InferenceGenerateResponse } from '../types/api.dto';
import { AxiosResponse } from 'axios';
import { getErrorMessage, AppException } from '../utils/error-handler';

interface LLMModelsResponse {
  models: string[];
}

interface LLMGenerateResponse {
  model: string;
  result: string;
  tokens: number;
  duration: string;
}

@Injectable()
export class InferenceService {
  private llmServiceUrl: string =
    process.env.LLM_SERVICE_URL || 'http://localhost:5000';

  constructor(@Inject(HttpService) private readonly httpService: HttpService) {
    console.log(
      '[InferenceService] Constructor - HttpService available:',
      !!httpService,
    );
  }

  getModels(): Observable<string[]> {
    console.log(`Fetching models from: ${this.llmServiceUrl}/inference/models`);
    return this.httpService
      .get<LLMModelsResponse>(`${this.llmServiceUrl}/inference/models`, {
        timeout: 5000,
      })
      .pipe(
        tap((response: AxiosResponse<LLMModelsResponse>) => {
          console.log('Models response:', response.data);
        }),
        map(
          (response: AxiosResponse<LLMModelsResponse>) =>
            response.data.models || ['llama2', 'mistral', 'neural-chat'],
        ),
        catchError((error: AppException | Error): Observable<string[]> => {
          const errorMessage: string = getErrorMessage(error);
          console.error(
            'Error fetching models from LLM service:',
            errorMessage,
          );
          console.warn('Returning default models due to LLM service error');
          return of(['llama2', 'mistral', 'neural-chat']);
        }),
      );
  }

  generateInference(
    prompt: string,
    model: string,
    context?: string,
  ): Observable<InferenceGenerateResponse> {
    console.log(
      `Generating inference with model ${model} at ${this.llmServiceUrl}/inference/generate`,
    );
    return this.httpService
      .post<LLMGenerateResponse>(
        `${this.llmServiceUrl}/inference/generate`,
        {
          prompt,
          model,
          context,
        },
        { timeout: 30000 },
      )
      .pipe(
        tap((response: AxiosResponse<LLMGenerateResponse>) => {
          console.log('Generation response:', response.data);
        }),
        map(
          (
            response: AxiosResponse<LLMGenerateResponse>,
          ): InferenceGenerateResponse => {
            const durationNum: number = parseInt(response.data.duration, 10);
            return {
              model: response.data.model,
              prompt,
              text: response.data.result,
              tokens: response.data.tokens,
              duration: isNaN(durationNum) ? 0 : durationNum,
            };
          },
        ),
        catchError(
          (
            error: AppException | Error,
          ): Observable<InferenceGenerateResponse> => {
            const errorMessage: string = getErrorMessage(error);
            console.error('Error generating inference:', errorMessage);
            console.warn('Returning mock inference due to LLM service error');

            const mockResponse: InferenceGenerateResponse = {
              model,
              prompt,
              text: `Mock response from ${model}: This is a simulated inference result for the prompt: "${prompt}". In a production environment, this would be generated by the actual LLM service.`,
              tokens: 42,
              duration: 150,
            };
            return of(mockResponse);
          },
        ),
      );
  }
}
