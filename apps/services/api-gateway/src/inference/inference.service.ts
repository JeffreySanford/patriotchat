import { Injectable, BadGatewayException } from '@nestjs/common';
import axios from 'axios';
import { InferenceModelsResponse, InferenceGenerateResponse, ApiError } from '../types/api.dto';

@Injectable()
export class InferenceService {
  private llmServiceUrl: string =
    process.env.LLM_SERVICE_URL || 'http://localhost:5000';

  async getModels(): Promise<string[]> {
    try {
      console.log(`Fetching models from: ${this.llmServiceUrl}/models`);
      const response = await axios.get<{ models: string[] }>(
        `${this.llmServiceUrl}/models`,
        { timeout: 5000 },
      );
      console.log('Models response:', response.data);
      return response.data.models || ['llama2', 'mistral', 'neural-chat'];
    } catch (error: unknown) {
      console.error('Error fetching models from LLM service:', error);
      console.warn('Returning default models due to LLM service error');
      // Return default models instead of throwing - allows demo to work without LLM service
      return ['llama2', 'mistral', 'neural-chat'];
    }
  }

  async generateInference(
    prompt: string,
    model: string,
    context?: string,
  ): Promise<InferenceGenerateResponse> {
    try {
      console.log(`Generating inference with model ${model} at ${this.llmServiceUrl}/generate`);
      const response = await axios.post<InferenceGenerateResponse>(
        `${this.llmServiceUrl}/generate`,
        {
          prompt,
          model,
          context,
        },
        { timeout: 30000 },
      );

      console.log('Generation response:', response.data);
      return response.data;
    } catch (error: unknown) {
      console.error('Error generating inference:', error);
      console.warn('Returning mock inference due to LLM service error');
      
      // Return mock response when LLM service is unavailable - allows demo to work
      return {
        model,
        prompt,
        text: `Mock response from ${model}: This is a simulated inference result for the prompt: "${prompt}". In a production environment, this would be generated by the actual LLM service.`,
        tokens: 42,
        duration: 150,
      };
    }
  }
}
